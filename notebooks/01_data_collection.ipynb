{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097c1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c885f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyAK91bbxUwvfGDd-oCuX09X8k6jHU1ANrU'\n",
    "\n",
    "# Apna College Channel ID\n",
    "CHANNEL_ID = 'UCBwmMxybNva6P_5VmxjzwqA'\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba414d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: Extract Video Metadata\n",
    "def get_channel_videos(channel_id, max_results=50):\n",
    "    \"\"\"\n",
    "    Extract all videos from a YouTube channel using pagination.\n",
    "    \n",
    "    Args:\n",
    "        channel_id: YouTube channel ID\n",
    "        max_results: Results per API call (max 50)\n",
    "    \n",
    "    Returns:\n",
    "        List of video dictionaries with id, title, and published_date\n",
    "    \"\"\"\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    page_count = 0\n",
    "    \n",
    "    print(f\"Starting video extraction from channel: {channel_id}\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # API request to search endpoint\n",
    "            request = youtube.search().list(\n",
    "                part='snippet,id',\n",
    "                channelId=channel_id,\n",
    "                maxResults=max_results,\n",
    "                order='date',  # Sort by publish date\n",
    "                type='video',  # Only get videos, not playlists\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            \n",
    "            response = request.execute()\n",
    "            page_count += 1\n",
    "            \n",
    "            # Extract video information\n",
    "            for item in response.get('items', []):\n",
    "                if item['id']['kind'] == 'youtube#video':\n",
    "                    video_data = {\n",
    "                        'video_id': item['id']['videoId'],\n",
    "                        'title': item['snippet']['title'],\n",
    "                        'published_date': item['snippet']['publishedAt'],\n",
    "                        'description': item['snippet']['description']\n",
    "                    }\n",
    "                    videos.append(video_data)\n",
    "            \n",
    "            print(f\"Page {page_count}: Extracted {len(response.get('items', []))} videos | Total: {len(videos)}\")\n",
    "            \n",
    "            # Check if there are more pages\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            \n",
    "            if not next_page_token:\n",
    "                print(\"\\nâœ“ All pages processed!\")\n",
    "                break\n",
    "            \n",
    "            # Be nice to the API - small delay between requests\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâœ— Error on page {page_count}: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5823e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: Process and Clean Data\n",
    "def process_video_data(videos):\n",
    "    \"\"\"\n",
    "    Convert video list to DataFrame and perform basic cleaning.\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame with processed data\n",
    "    \"\"\"\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(videos)\n",
    "    \n",
    "    # Convert published_date to datetime\n",
    "    df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "    \n",
    "    # Extract just the date (without time)\n",
    "    df['publish_date'] = df['published_date'].dt.date\n",
    "    \n",
    "    # Sort by date (newest first)\n",
    "    df = df.sort_values('published_date', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATASET SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Videos: {len(df)}\")\n",
    "    print(f\"Date Range: {df['publish_date'].min()} to {df['publish_date'].max()}\")\n",
    "    print(f\"Unique Titles: {df['title'].nunique()}\")\n",
    "    print(f\"Duplicate Titles: {len(df) - df['title'].nunique()}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: Basic EDA\n",
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on video metadata.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPLORATORY DATA ANALYSIS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # 1. Check for missing values\n",
    "    print(\"1. Missing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # 2. Videos per year\n",
    "    print(\"\\n2. Videos Published Per Year:\")\n",
    "    df['year'] = pd.to_datetime(df['published_date']).dt.year\n",
    "    year_counts = df['year'].value_counts().sort_index()\n",
    "    print(year_counts)\n",
    "    \n",
    "    # 3. Sample titles\n",
    "    print(\"\\n3. Sample Video Titles (First 5):\")\n",
    "    for idx, title in enumerate(df['title'].head(), 1):\n",
    "        print(f\"   {idx}. {title}\")\n",
    "    \n",
    "    # 4. Title length statistics\n",
    "    df['title_length'] = df['title'].str.len()\n",
    "    print(f\"\\n4. Title Length Statistics:\")\n",
    "    print(f\"   Average: {df['title_length'].mean():.0f} characters\")\n",
    "    print(f\"   Min: {df['title_length'].min()} | Max: {df['title_length'].max()}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a34706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "YOUTUBE VIDEO METADATA EXTRACTION\n",
      "Channel: Apna College\n",
      "============================================================\n",
      "\n",
      "Starting video extraction from channel: UCBwmMxybNva6P_5VmxjzwqA\n",
      "\n",
      "Page 1: Extracted 50 videos | Total: 50\n",
      "Page 2: Extracted 50 videos | Total: 100\n",
      "Page 3: Extracted 50 videos | Total: 150\n",
      "Page 4: Extracted 0 videos | Total: 150\n",
      "\n",
      "âœ“ All pages processed!\n",
      "\n",
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Total Videos: 150\n",
      "Date Range: 2021-09-20 to 2025-11-14\n",
      "Unique Titles: 142\n",
      "Duplicate Titles: 8\n",
      "\n",
      "============================================================\n",
      "EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. Missing Values:\n",
      "video_id          0\n",
      "title             0\n",
      "published_date    0\n",
      "description       0\n",
      "publish_date      0\n",
      "dtype: int64\n",
      "\n",
      "2. Videos Published Per Year:\n",
      "year\n",
      "2021    13\n",
      "2022    76\n",
      "2023    19\n",
      "2024    26\n",
      "2025    16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. Sample Video Titles (First 5):\n",
      "   1. Complete Full Stack Web Development Preparation : MERN Stack + 6 Major Projects | New Delta 8.0 ðŸš€\n",
      "   2. How he got placed in Japan as Software Engineer ? Learnt Japanese with DSA, Development\n",
      "   3. CISCO - 24LPA | Cisco Ideathon #results #placement #apnacollege\n",
      "   4. How to study for College Exams ? Just do this for best GPA!\n",
      "   5. How this student got selected for Google Summer of Code (GSoC) for Jenkins ? 3rd year BCA student\n",
      "\n",
      "4. Title Length Statistics:\n",
      "   Average: 58 characters\n",
      "   Min: 17 | Max: 103\n",
      "\n",
      "âœ“ Data saved to: ../data/apna_college_videos.csv\n",
      "\n",
      "Columns in dataset: ['video_id', 'title', 'published_date', 'description', 'publish_date', 'year', 'title_length']\n",
      "\n",
      "============================================================\n",
      "SAMPLE DATA (First 3 rows):\n",
      "============================================================\n",
      "   video_id                                                                                             title publish_date\n",
      "uNZwUdfU43Q Complete Full Stack Web Development Preparation : MERN Stack + 6 Major Projects | New Delta 8.0 ðŸš€   2025-11-14\n",
      "sMUWg_OlbbY           How he got placed in Japan as Software Engineer ? Learnt Japanese with DSA, Development   2025-11-11\n",
      "NX8F3QTXKc4                                   CISCO - 24LPA | Cisco Ideathon #results #placement #apnacollege   2025-10-11\n",
      "\n",
      "============================================================\n",
      "SCRIPT COMPLETED SUCCESSFULLY!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"YOUTUBE VIDEO METADATA EXTRACTION\")\n",
    "    print(\"Channel: Apna College\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Extract videos\n",
    "    video_list = get_channel_videos(CHANNEL_ID)\n",
    "    \n",
    "    if not video_list:\n",
    "        print(\"No videos found or error occurred.\")\n",
    "    else:\n",
    "        # Step 2: Process data\n",
    "        df = process_video_data(video_list)\n",
    "        \n",
    "        # Step 3: Basic EDA\n",
    "        df = perform_eda(df)\n",
    "        \n",
    "        # Step 4: Save to CSV\n",
    "        output_file = '../data/apna_college_videos.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nâœ“ Data saved to: {output_file}\")\n",
    "        print(f\"\\nColumns in dataset: {list(df.columns)}\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SAMPLE DATA (First 3 rows):\")\n",
    "        print(\"=\"*60)\n",
    "        print(df[['video_id', 'title', 'publish_date']].head(3).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCRIPT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
